{"componentChunkName":"component---src-pages-lines-index-tsx","path":"/lines","result":{"data":{"allMdx":{"nodes":[{"id":"158d55a3-27df-5276-9648-439b48f2fa7a","body":"\nI have been diving deeper into photography again for the first time since living in San Diego in 2022. In part,\ndue to the increased nerdiness of understanding and appreciating gear more, and in part due to being back in\nmy hometown of Rochester, NY. Perhaps it was inevitable that I would lean further into this hobby, being back\nin one of the forerunning cities of optics and film technologies. \n\n---\n\n![FUJI_160](./assets/2025-10__modified_fuji_160_unedited-19.jpg)\n\n---\n\n<details>\n<summary>gear</summary>\n\nHa, nice try, nerd.\n\n</details>\n\n<details>\n<summary>inspiration</summary>\n\n- [diggles](https://www.digglesphotography.com/blog/moments-for-photography-in-alaska-with-the-hasselblad-cfv-100c) for his eye and writing\n- [vmoldo](https://vmoldo.com/blog/) for his Lightroom development and educational series\n- [peter mckinnon](https://www.petermckinnon.com/about-bio) for his aesthetic (behind and in front of the lens)\n- [gregpack](https://gregpak.com/a-love-letter-and-guide-to-the-canon-ftb-best-first-film-camera-ever/) for his nerdiness and writing\n- [lomo](https://www.lomography.com/magazine/186424-diy-film-retriever-extractor) for its articles, and hosting platform\n\n</details>","fields":{"slug":"/photog/"},"frontmatter":{"date":"November 21, 2025","updated":null,"draft":null,"title":"photog","hidden":null,"tags":["nerd shit","hipster shit"]}},{"id":"c4df54f0-e233-5042-a2a3-d7ee7ff88880","body":"\n# It's Not Me, It's You\n\nFor context, back in 2021 I decided to switch my site from vanilla HTML + CSS (with a Jekyll blog under one path)\nto a Gatsby site. \n\n## The Appeal\n\nOur front-end team at work became pretty stacked, and I generally only tapped in to commit build, infra, or\nrepository hygiene updates. I was craving being able to touch React. I'd stumbled upon Hugo and Gatsby, wanted to\ntinker. Ultimately, Gatsby _seemed_ the most approachable of the two. Translation: its marketing worked on me.\n\nAt first, after adopting it, I thought that I just was rusty in the front-end space. I thought it was reasonable\nthat I spent more time digging through the four different doc sets required to get started and up and running than I\ndid writing React code or CSS.\n\n- Official Gatsby guides (to know what to do to rewrite my site with Gatsby)\n- Adjacent Gatsby references that the guides frequently link to (because I guess their official guides often\nneed supplemental help, and perhaps different Gatsby contributors had different ideas on how to structure docs?)\n- Gatsby plugin homepages/documentation (because Gatbsy seems to have thought federated was the way to go for \ncore site generator capabilities)\n- Front-end tooling documentation (because I am now supporting a React app to write personal website updates)\n\nThat's because, at some points, it felt kind of intruiging! I wasn't stuck hacking at HTML and CSS, I was spending\nmy time \"coding\"! Everything is smooth and simple in the end, after my first build with CI/CD:\n\n- To make blog posts, I write a new `.mdx`, push, et. voila!\n- To make site updates, I write React relatively quickly, push, et. voila!\n- ___To keep the site safe and up-to-date, I watch for `npm` warnings, errors, or security flaws___\n\n## The Downfall\n\nTwo years later, and that last bullet is killing me. Every time I sit down to upgrade all things Gatsby, I find myself\nin an annoying sinkhole of problems introduced by the framework itself. The latest snafu is what put the final nail in the\ncoffin.\n\n#### Updating from one minor version to another\n\nIt should be this easy:\n\n1. Update Gatsby and its plugins\n\n```shell\n$ npm update gatsby \\\n           gatsby-plugin-google-gtag \\\n           gatsby-plugin-image \\\n           gatsby-plugin-mdx \\\n           gatsby-plugin-sharp \\\n           gatsby-remark-images \\\n           gatsby-source-filesystem\n```\n\n---\n\n2. Update my toolchain\n\n```shell\n$ npm update -D eslint \\\n              eslint-config-prettier \\\n              eslint-plugin-prettier \\\n              eslint-plugin-react \\\n              prettier \\\n              typescript \\\n              @typescript-eslint/eslint-plugin \\\n              @typescript-eslint/parser\n```\n\nNeither of these commands work while Gatsby was pinned to `5.5.0`, and while all other dependencies at least\na minor version behind. Okay, I guess that's not the end of the world. But why won't they work? Because \nGatsby's core library has conflicting dev dependencies, so their developers are happy. Okay, I still \nunderstand. I will update by hand.\n\n---\n\n3. Update `package.json` with entirely new version numbers for everything, and `npm i`\n\nlol\n\n```shell\nnpm WARN ERESOLVE overriding peer dependency\nnpm WARN While resolving: eslint-config-react-app@6.0.0\nnpm WARN Found: @typescript-eslint/eslint-plugin@5.62.0\nnpm WARN node_modules/gatsby/node_modules/@typescript-eslint/eslint-plugin\nnpm WARN   @typescript-eslint/eslint-plugin@\"^5.60.1\" from gatsby@5.13.0\nnpm WARN   node_modules/gatsby\nnpm WARN     gatsby@\"^5.13.0\" from the root project\nnpm WARN     10 more (babel-plugin-remove-graphql-queries, ...)\nnpm WARN \nnpm WARN Could not resolve dependency:\nnpm WARN peer @typescript-eslint/eslint-plugin@\"^4.0.0\" from eslint-config-react-app@6.0.0\nnpm WARN node_modules/gatsby/node_modules/eslint-config-react-app\nnpm WARN   eslint-config-react-app@\"^6.0.0\" from gatsby@5.13.0\nnpm WARN   node_modules/gatsby\nnpm WARN \nnpm WARN Conflicting peer dependency: @typescript-eslint/eslint-plugin@4.33.0\nnpm WARN node_modules/@typescript-eslint/eslint-plugin\nnpm WARN   peer @typescript-eslint/eslint-plugin@\"^4.0.0\" from eslint-config-react-app@6.0.0\nnpm WARN   node_modules/gatsby/node_modules/eslint-config-react-app\nnpm WARN     eslint-config-react-app@\"^6.0.0\" from gatsby@5.13.0\nnpm WARN     node_modules/gatsby\nnpm WARN ERESOLVE overriding peer dependency\nnpm WARN While resolving: eslint-config-react-app@6.0.0\nnpm WARN Found: @typescript-eslint/parser@5.62.0\nnpm WARN node_modules/gatsby/node_modules/@typescript-eslint/parser\nnpm WARN   @typescript-eslint/parser@\"^5.60.1\" from gatsby@5.13.0\nnpm WARN   node_modules/gatsby\nnpm WARN     gatsby@\"^5.13.0\" from the root project\nnpm WARN     10 more (babel-plugin-remove-graphql-queries, ...)\nnpm WARN   1 more (@typescript-eslint/eslint-plugin)\nnpm WARN \nnpm WARN Could not resolve dependency:\nnpm WARN peer @typescript-eslint/parser@\"^4.0.0\" from eslint-config-react-app@6.0.0\nnpm WARN node_modules/gatsby/node_modules/eslint-config-react-app\nnpm WARN   eslint-config-react-app@\"^6.0.0\" from gatsby@5.13.0\nnpm WARN   node_modules/gatsby\nnpm WARN \nnpm WARN Conflicting peer dependency: @typescript-eslint/parser@4.33.0\nnpm WARN node_modules/@typescript-eslint/parser\nnpm WARN   peer @typescript-eslint/parser@\"^4.0.0\" from eslint-config-react-app@6.0.0\nnpm WARN   node_modules/gatsby/node_modules/eslint-config-react-app\nnpm WARN     eslint-config-react-app@\"^6.0.0\" from gatsby@5.13.0\nnpm WARN     node_modules/gatsby\nnpm WARN ERESOLVE overriding peer dependency\nnpm WARN While resolving: react-server-dom-webpack@0.0.0-experimental-c8b778b7f-20220825\nnpm WARN Found: react@18.2.0\nnpm WARN node_modules/react\nnpm WARN   react@\"^18.2.0\" from the root project\nnpm WARN   10 more (@gatsbyjs/reach-router, @mdx-js/react, gatsby, ...)\nnpm WARN \nnpm WARN Could not resolve dependency:\nnpm WARN peer react@\"0.0.0-experimental-c8b778b7f-20220825\" from react-server-dom-webpack@0.0.0-experimental-c8b778b7f-20220825\nnpm WARN node_modules/gatsby/node_modules/react-server-dom-webpack\nnpm WARN   react-server-dom-webpack@\"0.0.0-experimental-c8b778b7f-20220825\" from gatsby@5.13.0\nnpm WARN   node_modules/gatsby\nnpm WARN \nnpm WARN Conflicting peer dependency: react@0.0.0-experimental-c8b778b7f-20220825\nnpm WARN node_modules/react\nnpm WARN   peer react@\"0.0.0-experimental-c8b778b7f-20220825\" from react-server-dom-webpack@0.0.0-experimental-c8b778b7f-20220825\nnpm WARN   node_modules/gatsby/node_modules/react-server-dom-webpack\nnpm WARN     react-server-dom-webpack@\"0.0.0-experimental-c8b778b7f-20220825\" from gatsby@5.13.0\nnpm WARN     node_modules/gatsby\n```\n\nFine, whatever, I don't want to use Gatsby's version of ES Lint plugins or TypeScript plugins. It's my site, not\na Gatsby commit that I'm writing changes for. Okay, I'll ignore these errors.\n\n--- \n\n4. Get yelled at about vulnerabilities\n\n```shell\n14 moderate severity vulnerabilities\n\nTo address issues that do not require attention, run:\n  npm audit fix\n```\n\nYep:\n\n```shell\n$ npm audit fix\n```\n\nThere are still 12 moderate severity vulnerabilities.\n\nOkay, still a ton of warnings that I don't need to actually care about, but will need to read each time I revisit\nMy site's dependencies. \n\nFun stuff.\n\n---\n\n5. Does the app build now, though?\n\n```shell\n$ npm run build # (this is my script for a `gatsby build`, per their framework behavior)\n\n...build output...\n\n_Multiple instances of_:\n\n ERROR  UNKNOWN\n\n(node:12600) [DEP0040] DeprecationWarning: The `punycode` module is deprecated. Please use a userland alternative instead.\n(Use `node --trace-deprecation ...` to show where the warning was created)\n\n...build output...\n```\n\n__Shit, why?__\n\n```shell\n$ npx npm-why punycode\n```\n\nOh, okay. 3106 lines of output. Each Gatsby dependency I leverage consumes `punycode` through its dependencies.\nNice. \n\n#### Considering My Options\n\nAt this point, I'm regretting having gone down the mental road of choosing \"dev friendliness\" over site stability\nand maintenance. By not owning Gatsby, I have the luxury of simply using its features, but I also have the side-effect\nof not having any control over my own developer experience or my own site's stability in real time.\n\nI can't tell if I'm happy about this realization, or nervous that I've become one of _them_:\n\n- [Why You Should Write Your Own Static Site Generator](https://news.ycombinator.com/item?id=38126210)\n- [Why I built my own static site generator](https://news.ycombinator.com/item?id=28837760)\n- [I Built My Own Shitty Static Site Generator](https://news.ycombinator.com/item?id=25227181)\n- [Show HN: Blades - fast static site generator written in Rust](https://news.ycombinator.com/item?id=24760528)\n- [\"Make\" as a static site generator](https://news.ycombinator.com/item?id=37454853)\n\n## In The End\n\nI can continue relying on Gatsby for super easy minor updates to my web pages. I cannot rely on it for the very\n\"magic\" that turned me on to it: A simple SSG that offers a great dev experience.\n\nWhen my own developer configs \ncreate endless warnings and errors from the most essential part of Gatsby: its position in the JavaScript ecosystem,\nI can't help but feel it has lost its edge. I drank the Kool Aid. Now I have a belly ache.\n\n\n---\n\n---\n\n##### P.S.\n\nThis ended up being my first blog post in a while, and one that includes a lot of nested code blocks.\nTo improve readability, I wanted to adopt syntax highlighting and a background color for contrast against \"normal\ntext\". There was an update that was at one point compatible with `gatsby-plugin-mdx` via `gatsby-remark-highlight-code`\nthat now breaks with the latest version of Gatsby (`5.13.0` at time of writing). The workarounds include:\n\n- Using the MDX rehype-highlight library directly, and injecting it into the `gatsby-plugin-mdx` `options.rehypePlugins`\nfield\n  - This now requires `require(\"rehype-highlight\")`\n    - This is no longer compatible with the latest version of Gatsby, and these workarounds break:\n      - Asynchronously promise the Gatsby module config to dynamically `import rehypeHighlight`\n        - (breaks the Gatsby config import entirely)\n      - Using babel to create gatsby-config.js as a module \n        - (breaks the GraphQL library during build phase)\n\n- Writing an entire MDX parser to look for code blocks, and apply custom CSS\n  - This can employ the help of other libraries, like Prism, to do the styling\n  - If this were started, why stop? Why not extend the parsing to a larger engine? It could be some sort of content parser\n  plus content renderer. It could generate content based on some templated files. It could create an entire static site \n  based on the contents of a folder...\n\nOh...","fields":{"slug":"/bye-gatsby/"},"frontmatter":{"date":"December 21, 2023","updated":null,"draft":null,"title":"Gatsby, We Need To Talk","hidden":null,"tags":["nerd shit","editorial"]}},{"id":"08ed4a4a-db03-56fa-abc4-5b9ada534f39","body":"\nBelow are helpful commands I don't want to forget for Redshift admin and use. Those interested in a small torching\nof Redshift as a product are welcome to read the [sibling post](./redshift-opinion).\n\n---\n## Querying\n\n### A Distinct \"`DISTINCT`\"\n\n**PostgreSQL** users might be familiar with the ability to use `DISTINCT` as an expression evaluator, rather\nthan a clause, to reduce the result set down to records that only include the value evaluated by the\nexpression.\n\n**MySQL** users might be used to leveraging `GROUP BY` in a different way than normal\nto do the same.\n\nI.E: if I have a table, `orders`:\n<table border={1} frame={\"outside\"}>\n    <tr>\n        <th>id</th>\n        <th>customerId</th>\n    </tr>\n    <tr>\n       <td>1</td>\n       <td>1</td>\n    </tr>\n    <tr>\n        <td>2</td>\n        <td>1</td>\n    </tr>\n    <tr>\n        <td>3</td>\n        <td>1</td>\n    </tr>\n</table>\n\nOur first customer was keeping the lights on, and proved that the `orders` table has a many:one\nrelationship with the `customers` table.\n\nSuppose we need to pull the latest order for each customer, in a world where there is not just\na sole customer placing orders. Additionally, the context of the report involves many other fields are being\naggregated across dozens of other tables. `LIMIT 1` is not coming to the rescue.\n\nPostgres allows for:\n```sql\nSELECT\n    DISTINCT ON(customerId) customerId,\n    id\nFROM orders\nORDER BY customerId, id DESC;\n```\n\nOkay, cool. Weird, but cool. The developer specifies they _only_ want unique `customerId` values\nin their results, but want the respective order information along with it. Postgres constructs\ngroups based on the argument provided to `DISTINCT ON`, then omits any records that break the unique\nvalue specified. The `ORDER BY` gives the developer control over precedence on which distinct records\nto honor.\n\nMySQL perhaps helps visualize this better, literally making the developer create the groups themselves:\n`ORDER BY`:\n```sql\nWITH disctinct_customer_id_orders AS (\n    SELECT\n        id\n    FROM orders\n    GROUP BY customerId\n)\nSELECT * FROM distinct_customer_id_orders ORDER BY id DESC;\n```\n\nIt's still not perfect, either, because the `ORDER BY` should happen in the first query, but hey, MySQL tries.\n\n#### Redshift does neither...\n\n...yet I don't hate how Redshift achieves the same outcome, strictly from the developer\nexperience side of things:\n```sql\nSELECT\n    *\nFROM (\n    SELECT\n        id,\n        customerId,\n        RANK() OVER (PARTITION BY customerId ORDER BY id DESC) AS customer_id_rank\n    FROM orders\n) as ranked_orders\nWHERE ranked_orders.customer_id_rank = 1;\n```\n\nWhere Redshift could have continued reinventing an already finished, rehashed,\nre-spoked wheel of relational-algebra powered query engines, it instead took the modest route.\n\nThe developer is welcome to rewrite `GROUP BY` on Amazon's behalf. Also, the developer gets a bonus\ncolumn in their toolkit to play with. It's okay that the result set grew though, because it's powered\nby a warehouse meant for Big Data!\n\n## Administering\n\n### Groups & Users\n```sql\n-- Create groups\nCREATE GROUP MY_GROUP [WITH USER (user1, user2,...)];\n\n-- View all user groups (as cluster admin)\nSELECT * from PG_GROUP;\n\n-- View all users in a group\nSELECT usename FROM pg_user, pg_group\nWHERE pg_user.usesysid = ANY(pg_group.grolist)\n  AND pg_group.groname = '<group-name>';\n\n-- Apply to entire schema\nGRANT ALL ON SCHEMA my_schema TO GROUP my_group;\n\n-- Apply to specific tables\nGRANT SELECT ON TABLE my_table TO my_user;\n\n-- Apply read-only access to specific (or all) tables in a single schema\nGRANT SELECT ON ALL TABLES IN SCHEMA my_schema TO my_user;\n\n-- Create user\nCREATE USER my_user PASSWORD '<pass-string>';\n\n-- Create user under specific group\nCREATE USER my_user IN GROUP my_group PASSWORD 'pass';\n\n-- Alter existing user\nALTER USER my_user PASSWORD 'new-pass';\n```\n\n### Cluster Metadata\n```sql\n-- View tables within a Schema\nselect distinct tablename from pg_table_def where schemaname = '<schema-name>';\n\n-- View all schemas in a cluster\nselect * from pg_namespace;\n\n-- View connection activity within a cluster\nselect * from stl_connection_log\nwhere recordtime > '2021-02-07 00:00:00' and username in ('user1', 'user2')\norder by recordtime DESC;\n```\n","fields":{"slug":"/redshift-notes/"},"frontmatter":{"date":"May 9, 2023","updated":null,"draft":null,"title":"Redshift Notes","hidden":null,"tags":["nerd shit"]}},{"id":"a76bcedf-4430-5b4e-ab85-03f80cb6b904","body":"\n# Redshift: Body Hiding as a Service\n\nWhen I started writing this post, it was to be a single, helpful guide for myself. Before long, it turned into a pretty\ngnarly rant about Redshift as a product, rather than a truly marvelous technical solution. The helpful guide still does\nexist, for what it's worth. Check it out as a [sibling post](./redshift-notes).\n\nIn full disclosure: I do work with an organization that has adopted Redshift as an internal tool for various reporting\npurposes, as of mid-to-late 2021. In hindsight, the decision to bring it in as a tool likely had little to do with\nits prowess and capabilities, and more to do with certain personnel confusing the term \"growth\" for \"infinite amounts of\ncash to blow\".\n\nWe leveraged I/O as follows:\n<table className={\"three-column\"} border={1} frame={\"void\"} rules={\"rows\"}>\n    <tr>\n        <th>Frequency</th>\n        <th>Operation</th>\n        <th>Mechanism</th>\n    </tr>\n    <tr>\n        <td>Once</td>\n        <td><b>WRITE</b> all tables from external, primary datasource(s)</td>\n        <td>AWS DMS + Vendor integration</td>\n    </tr>\n    <tr>\n        <td>Ad-hoc</td>\n        <td><b>WRITE</b> custom views to cache aggregate data used solely for reporting purposes</td>\n        <td>AWS Redshift Console + DBMS Tools</td>\n    </tr>\n    <tr>\n        <td>Ad-hoc</td>\n        <td><b>WRITE</b> DDL changes that have occurred on the primary datasource(s)</td>\n        <td>AWS DMS</td>\n    </tr>\n    <tr>\n        <td>Ad-hoc</td>\n        <td><b>READ</b> from various tables and views</td>\n        <td>DBMS Tools + other query tools</td>\n    </tr>\n    <tr>\n        <td>Regularly</td>\n        <td><b>READ</b> from various tables and views</td>\n        <td>Scheduled, and systemic processes</td>\n    </tr>\n</table>\n\nThose `WRITE` operations are inevitable, regardless of the destination data source, sure. But they specifically require\ngoing through AWS managed toolchains, either S3 for a `COPY` or via AWS Database Migration Service for table mapping\nor other ETL jobs.\n\nAllow me to pause my uppity browbeating for a moment. I personally find the tooling behind Redshift to be badass. It:\n- is run on its own custom flavor of SQL\n- can [allegedly] act as a decent \"big-data\" solution, offering scaling up to PB of data quickly\n- comes with training wheels:\n    - a phenomenal documentation layer\n    - a web-based query console\n\nBeing badass never lets one off the hook, though. I'm sure you could ask Bezos.\n\n## PaaS vs. DBaaS\nLook at those `WRITE`s again. None of them are from an SDK. None of them are from a code base.\nIn fact, it seems that (obligatory: _at the time of writing this_) Redshift really still doesn't cater to\ndevelopment teams as a primary data source for server applications.\n- Its SDKs cater to managing Redshift clusters and its assets, rather than invoking queries\n- The official Data API documentation includes instructions on how to map result sets by hand to JDBC objects\n- There is no official supported ORM for Redshift\n- The client-side SDK seems to be exactly what it would have taken to construct the Amazon internal cluster management\nUI within the AWS management console\n\nI'm not trying to build a case that Redshift was at all designed for the purpose of acting as a server application's\nprimary datasource. It's important to acknowledge that it's a profoundly accessible data storage solution with seemingly\ngreat potential for scale, and capability of catering to various use-cases. Oh, also, by accessible I mean, \"widely\nadoptable for those with a credit card burning a hole in their pocket.\"\n\nHowever, I think it seems obvious as to _why_ this purpose was not its goal in the first place. In fact, recalling the\nI/O operations I expressed above, it's truly no mystery as to why there are no official Amazon-led connectors/SDKs/ORM\nlibraries that bind any particular language to Redshift capabilities:\n\n**_If the programmatic capabilities a developer has with a platform are to create and manage new infrastructure, then\nthe platform remains a platform to be managed, rather than a tool to be leveraged._**\n\nIt might be in Amazon's best interest, strictly from a margins and maintenance cost perspective, that Redshift remain a\npure PaaS product. Unfortunately, when developers (and not data scientists) are the ones responsible for the operations\ninvolving Redshift I/O, it becomes painfully clear that it's simply a SQL database with a bunch of added cost.\n\n## DBaaS...\nAmazon's own Relational Database Service (RDS) contrasts with Redshift in ways that really helped me arrive to my\nconclusion.\n\n1. RDS virtual instances have smaller downtime on average over Redshift clusters during re-scaling/elastic resizing\n2. RDS is cheaper, on average, per use case (_at the time of writing_)\n   1. An RDS `db.t3.large` (1 core, 2vCPUs, 8GB RAM) is $0.136/hour, vs. the minimal cluster charge of $0.25/hour on Redshift\n3. Allegedly, Redshift's query optimizer, famous for making its \"big data\" capabilities a reality, make it slower on average\nthan RDS for queries under the several million record count\n\nRedshift was marketed well, and perhaps even had the best of intentions to disrupt the market of big data tools. It\ncertainly smells like a big custom SQL database cloud with purposeful dependencies that hike the cost of using it\neffectively. It seems that it is most effectively serving entities who have so much data to play with that dumping it\nin a central place to stream outwards in creative ways. I hope their pockets are deep. So does Amazon, I'm sure.\n\n## References\n- [Redshift Rank](https://docs.aws.amazon.com/redshift/latest/dg/r_WF_RANK.html)\n- [Redshift Data API](https://drive.google.com/file/d/1IliGkQIwR0h3EjXoA9VysJm9QIqdBKGN/view?usp=share_link)\n- [Redshift SDK Examples](https://drive.google.com/file/d/18UDxpeWf7nJSs01qHaGm0-MW8PMEVTdg/view?usp=share_link)\n- [Redshift Java SDK Overview](https://drive.google.com/file/d/1nG9r9bQvD8NN_m22jpk2J2KoxY6ggKej/view?usp=share_link)\n- [Redshift Python SDK Overview](https://drive.google.com/file/d/1ZlVAF3TvrXLSb-V4jGkc89RJCfXmwSVO/view?usp=share_link)\n- [Redshift Client-side SDK](https://drive.google.com/file/d/1LwYngIharMybl_PHUEQzvyvsT3YGP3oa/view?usp=share_link)\n- [RDS vs. Redshift Analysis](https://drive.google.com/file/d/19cm58920poSyw_Z3QwvmPKG57bvlqhwa/view?usp=share_link)\n   - This is from [Hevo](https://hevodata.com/), an ETL and \"data pipeline\" platform. Not necessarily a competitor to, and\nlists Redshift as a source they work with out of the box. Take that as you will...\n","fields":{"slug":"/redshift-opinion/"},"frontmatter":{"date":"May 9, 2023","updated":null,"draft":null,"title":"Redshift, Editorial","hidden":null,"tags":["nerd shit","editorial"]}},{"id":"6b982630-1a0f-5b0f-8ee5-f70fde15914f","body":"\n# MySQL - Helpful Tidbits\n\nMy experiences at Rev360 (RevolutionEHR) and Mercato brought about developing for back-ends that leverage MySQL for data storage solutions. All my other previous experiences were within the context of Oracle, DB2, and SQL Server database technologies, and I found myself constantly Googling to see how to do things better in MySQL. Below are some of those findings.\n\n## Perform batch UPDATEs with a temp table\n\nWhen performing `UPDATE`s against a table in bulk, MySQL is pretty non-performant when given a raw ```UPDATE``` statement with a ```WHERE``` clause:\n\n```sql\nUPDATE A\nSET foo = 'bar' WHERE id < 1000;\n```\n\nThe best way of handling bulk ```UPDATE```s turns out to be establishing a temp table containing the values that encapsulate the data for the ```UPDATE```, and then performing the ```UPDATE``` with a ```JOIN``` against the temporary table.\n\n```sql\nDROP TABLE IF EXISTS `tempFooValues`;\n\nCREATE TABLE `tempFooValues` (\n`id` INT(11) UNSIGNED NOT NULL,\n`foo` VARCHAR(255) NOT NULL,\nPRIMARY KEY(`id`)\n);\n\nINSERT INTO tempFooValues (id, foo) VALUES\n(1, 'bar'),\n(2, 'bar'),\n...,\n...,\n(1000, 'bar');\n\nUPDATE A\nINNER JOIN tempFooValues ON tempFooValues.id = A.id\nSET\nA.foo = tempFooValues.foo;\n```\n\n## Leveraging CASE within a JOIN\n\nThis one is pretty self-explanatory...\n\n```sql\nSELECT * FROM A\nJOIN B\nON CASE\nWHEN B.type IN (1, 3) AND B.a_id = A.id THEN 1\nWHEN B.type IN (2) AND B.other_id = A.other_id THEN 1\nELSE 0\nEND = 1;\n```\n\n## Converting Timezones from UTC\n\nThe arguments for the \"from\" and \"to\" timezones in `CONVERT_TZ` are the offsets from\nUTC. Therefore, `+00:00` is itself UTC, and `-07:00` would equate to PT in the U.S.\n\n```sql\nSELECT CONVERT_TZ(created, '+00:00','-07:00') FROM users;\n```\n\n## Performing Maintenance on Huge Tables\n\nOld faithful has been holding down the fort for 9 years. It's full of juicy data that's\nboth important for reporting and critical for application business logic for end users.\nIt is also missing a ton of helpful index values underneath the columns matter most.\n\nYour reports won't report, and you know in your gut that `ALTER TABLE` in real time on \nold faithful will take about 10,000 years to execute. \n\nSo, you perform the following:\n- Create a new table that contains the index values you require\n- Copy the data from old faithful into this clone table with indices\n- Rename (or drop) the original old faithful, and rename the clone to become old faithful\n\n```sql\nBEGIN;\n\nCREATE TABLE IF NOT EXISTS oldFaithfulWithIndices LIKE oldFaithful;\n\nALTER TABLE oldFaithfulWithIndices\n    ADD INDEX `account_id_index`(`account_id`),\n    ADD INDEX `email_address_index`(`email_address`),\n    ADD INDEX `created_datetime_index`(`created_datetime`),\n    ADD INDEX `updated_datetime_index`(`updated_datetime`);\n\nINSERT INTO oldFaithfulWithIndices SELECT * FROM oldFaithful;\n\nRENAME TABLE oldFaithful TO oldFaithful_archive;\n\nRENAME TABLE oldFaithfulWithIndices TO oldFaithful;\n\nCOMMIT;\n```\n\n\n## Adding composite unique indexes to existing table\n\nWhen working with a table that already contains data that would violate a desired `unique` index\nyou wish to apply to a table, it's important you scrub the dupes.\n\n```sql\nDELETE FROM my_table\nWHERE id NOT IN (\n    SELECT MIN(id)\n    FROM my_table\n    GROUP BY col_1, col_2\n)\nAND (col_1, col_2) in (\n    SELECT col_1, col_2\n    FROM my_table\n    GROUP BY col_1, col_2\n    HAVING count(*) > 1\n);\n```\n\n## GROUP_CONCAT()\n\nThe `GROUP_CONCAT()` function is used to aggregate the results of a column into a single field.\n\nUse cases...\n\nConcatenating all single-column results into a single field results:\n\n```sql\nSELECT GROUP_CONCAT(email_address) FROM customer; \n```\n\nGrouping all results of a column into a single field relative to a shared group identifier\n\n```sql\nSELECT customer, GROUP_CONCAT(order.id)\nFROM customer\nJOIN order ON customer.id = order.customer_id\nGROUP BY customer.id\n```\n\n`GROUP_CONCAT()` allows for customizability of the structure of the aggregated data:\n\n```sql\nSELECT customer, GROUP_CONCAT(order.id ORDER BY order.id DESC SEPARATOR '| ')\nFROM customer\nJOIN order ON customer.id = order.customer_id\nGROUP BY customer.id\n```\n\n## `LAST_INSERT_ID()`\n\nFor any given session in the database engine, the session user can inspect their latest insertion ID with:\n\n```sql\nSELECT LAST_INSERT_ID();\n```\n\nThis function operates on a per-connection basis, meaning that when the DB session user terminates their\ncurrent connection, their next session will yield a different result. A fresh session yields `0`. ","fields":{"slug":"/mysql-tips-and-tricks/"},"frontmatter":{"date":"January 5, 2021","updated":"April 3, 2025","draft":null,"title":"MySQL Tips and Tricks","hidden":null,"tags":["nerd shit"]}},{"id":"7baaa587-1819-581d-be49-0b2cdcf8a074","body":"\n# The Deck Itself\n## [git-intermediate](https://steebe.github.io/git-intermediate-deck/#/)\n\n# Teaching Git\nWhile working for a large company whose technological aspirations were dwarfed by its six-tiered leadership's inability to understand technology, I found myself in a position that should have annoyed any developer.\n\nTeaching is luckily something I _found_ enjoyable at one point in my life; I was fortunate enough to have the means and opportunity to be a teaching assistant (TA) and student assistant (SA) at my alma mater, [The University at Buffalo](http://www.buffalo.edu/). My duties as a TA taught me patience, discipline, and understanding. I didn't anticipate I'd have to tap into those attributes so heavily in an industry setting day-to-day, but nevertheless I found myself having to harness them simply to channel some empathy for a group of developers who had no knowledge (or interest for that matter) in proper version control & repository hygiene.\n\nIf I had to take a stab in the dark, I would guess that of every five developers I've had the opportunity to chat with, two have experienced working with the kinds of colleagues I'm describing. Perhaps you know of them too, the developers that...\n* Email zip files of the latest state of their local repository to the entire team\n* Push directly to master, or a protected branch, without review\n* Never establish a protected branch\n* Perform merges of a single branch that contains over 100 new commits\n* Introduce sensitive credentials or other secrets to the remote repository, and then simply delete the repository weeks later after realizing the mistake\n** ...two to three times a quarter\n\nAfter a few weeks of slowly losing my sanity while watching these examples unfold in repositories I was meant to contribute to, I felt that I didn't need to come to grips with this reality. Instead, I approached my management with a simple, yet stern request for new repositories that I would own. I locked down master, published README.md files with strict development requirements (checkstyles for Java, husky TS Lint checks for JS projects, code coverage requirements, the whole nine...), and wrote & conducted the presentation found below. I even came up with an [example repository](https://github.com/steebe/git-intermediate-practice) with pre-configured branches that would allow those who cloned it to test all of the commands I introduce in the presentation.\n\nI set up a webhook on the example repository's remote clone operatiton to notify me when individuals cloned it for practice. Not a single hit.\n\nEmails continued pouring into my inbox containing zipped up repositories in a dirty state with changes.\n\n\"Architects\" continued bastardizing their \"protected\" branches.\n\nI didn't lose my sanity, though, and I didn't give in to malpractice. If you find yourself in a frustrating situation that requires a bit of back-pedalling, please feel free to steal this in an effort to preserve even more sanity.\n\n## [git-intermediate](https://steebe.github.io/git-intermediate-deck/#/)\n### Made with [reveal.js](https://revealjs.com)\n","fields":{"slug":"/git-intermediate/"},"frontmatter":{"date":"April 4, 2020","updated":null,"draft":null,"title":"Git: Intermediate Commands","hidden":null,"tags":["nerd shit"]}},{"id":"b7436c46-2dd2-5e5e-bd35-a6c933b2797e","body":"\nBelow is a post from November, 2017, that I ghost-wrote for [Morningside Translations](https://www.morningtrans.com/html5-how-a-developers-dream-turned-into-a-localization-engineers-good-fortune/). This translation services conglomerate had just acquired my father's company, Advanced Language Translation one year prior, and he must have been feeling too important to be stuck in the weeds blogging about web technologies.\n\n# HTML5: How a Developer’s Dream Turned into a Localization Engineer’s Good Fortune\n\nLast month, the newest HTML standard from the World Wide Web Consortium (W3C) officially turned three years old. However, in a trend similar to most programming languages or front-end frameworks, HTML5 is only now starting to get the global use it deserves.\n\nIts release three years ago sparked the usual curiosity from UX designers and web programmers, but now the development community is not the only audience excited about HTML5’s capabilities. Business analysts, small business owners, localization engineers, social media marketers, and search engine optimizers are all admitting that their companies are making great strides to become HTML5 compliant.\n\nThe release of this new “Living Standard,” as many are calling, has made content localization incredibly easy and immensely flexible for both creators and consumers on the web.\n\n![HTML5_LOGO](./assets/html5.png)\n\n\n## Keeping it simple\nThe most brilliant aspect of this move by W3C is its simplicity – technical updates are brief, intuitive and powerful. Sites that leverage HTML5 are now free from worrying over the differences between devices, browsers and the content the site itself owns. For the first time in web history, HTML markup itself can handle a shift between mobile and desktop without the aid of a larger styling framework like Bootstrap to handle the legwork.\n\nBeyond that, HTML5 introduced a rich and robust API for natively hosting playback for video, audio content and vector-graphic animation in a site’s static code. By now, it’s not exactly news that HTML5 was the final nail in Flash’s coffin. The technical benefits, however, are just the tip of the iceberg.\n\n\n\n## Introducing New elements\n\nMost aspects of the new standard that appeal to global content creators are primarily simple additions to the markup language’s element and attribute library. HTML5 offers new tags that allow for a logical organization of content based on type. For example, an entire block of elements dedicated to blogs and articles was given support.\n\nContent creators now have a slew of native HTML elements to choose from to organize posts or informative pages within their domain. The gist of the elements `<article/>`, `<section/>`, `<header/>` and `<footer/>` is still simply a method of storing plain text in a document, but with better out-of-the-box styling and the ability to localize more effectively as compared to `<span/>` and `<p/>`. A developer may work with their business team to build requirements around placing all translatable content in `<article/>` elements, for instance. Other examples of elements that display HTML5’s modularity are `<summary/>`, `<section/>` and `<description/>`, whose purposes are fairly self-explanatory.\n\n\n\n## Attributes\n\nThe only ironic part about adding new element sets to allow for content encapsulation is that a smaller component of the markup – attributes – is proving to be more powerful for people worrying about localization. Attributes now allow for any element that houses text to be tagged to identify:\n\n* The source language of the text\n* Whether or not it contains a term that should be translated\n* The intended directionality of the text.\n\nWhile new modular element sets may help with organization, attributes such as these allow for both increased developer freedom and seamless localization capabilities.\n\n\n\n### Translation just got a whole lot easier\n\nThe concept of if a particular text block should be translated is huge; now the parsing engines of translation services can get by with filtering markup on one of two attributes: translate and its-term. The former’s purpose is to indicate if a medium-to-large body of text requires translation, and the latter is for indicating terminology definitions within a body of text.\n\nThe lang attribute is another simple addition that allows for indicating which source language is contained in an element. Pairing lang with the dir attribute has the potential to leave zero gaps in requirements when a content creator approaches a vendor for localization. The dir attribute, standing for “directionality,” explains whether the source language is right-to-left or vice versa.\n\nA perhaps less obviously named, but just as powerful feature that appeared with HTML5 is the ruby annotation. The elements that the ruby annotation is comprised of allow for specifying the pronunciation of the text the annotation encapsulates. While this is applicable to all languages, it marks the first time that Asian languages have the web support for full coverage of their metadata. Gone are the days of “difficult languages” when performing localization and desktop publishing on web content.\n\n\n\n## A tool of revolution\n\nHTML5 is the small and clairvoyant technical change that allowed for a huge step in the right direction in terms of globalizing web content. Beyond being a sturdy addition to a crucial software construct, it has already acted as a tool of revolution in the way companies think of their presence on the Web. Hopefully, over the next three years we will see the frameworks that are leveraging its sleek and lucid API make it easier for brilliant business minds to reach a wider audience with more accurate localization capabilities.\n","fields":{"slug":"/html5-and-localization/"},"frontmatter":{"date":"August 3, 2019","updated":null,"draft":null,"title":"HTML5 and Localization","hidden":null,"tags":["nerd shit"]}},{"id":"5d8927ed-d0f9-55b8-8078-ed6a73a05dd7","body":"# How Privatized Healthcare is Just Universal Healthcare That's Ripping You Off\n\nThe following post essentially represents a series of observations regarding the current healthcare system in the United States that have manifested themselves within my deep psyche. I don't pretend to be a medical professional, but then again it doesn't take one to note the ins and outs of an economic powerhouse from the outside. This post is not comprehensive, nor should it be seen as an attempt at a hint at policy building of any kind; I don't pretend to be a politician either.\n\n## How it works\n\nIn figure 1-1, the following phenomena is displayed:\n\n    A.) Employers and Individuals contribute to various companies within the\n    private Healthcare entity*. The prices that are paid are based on three\n    factors:\n        1. The plan chosen\n        2. A formulaic cost of the plan, calculated by various statistics**\n        aggregated by the company\n        3. The margins the company wishes to make (this should scare most, except for healthcare companies, that is)\n    B.) The companies within this entity pay the healthcare costs of their\n    covered individuals that require services.\n\n    * The \"entity\" in this case being the collection of all private healthcare providers in the country.\n    ** How much the entity will have to pay out for patient coverage in any given block of time. I don't think it's a stretch to say this number is likely inflated or an egregious miscalculation in favor of the entity.\n\nFigure 1-1\n![1-1](./assets/healthcareless_1-1.png)\n\nThink with me here: at a _very_ high level, the private healthcare system is a black box whose input\nis cash and whose output is far less cash. It's a highly profitable entity whose\nrevenue is directly attributed to the statistics that only a minute few of the\nparticipating \"customers\" will require services that cost more than their\nindividual contribution to the revenue.\n\n## Whom does it benefit?\nFrom the surface, it seems that the primary beneficiaries (yes, pun intended) of a system like the one described above are as follows:\n- The companies that make up the private healthcare entity (see their increased [profitability indexes](https://csimarket.com/Industry/industry_Profitability_Ratios.php?s=800))\n  - Via plans to obtain bigger tax breaks [through lobbying](https://www.followthemoney.org/research/institute-reports/health-insurance-companies-give-healthy-donations-to-political-campaigns)\n  - Via \"minimizing operational costs,\" which is fancy marketing speak for automating as much of their lower clerical workforce as possible\n  - Via [subsidies](https://www.bloomberg.com/news/articles/2018-05-23/it-costs-685-billion-a-year-to-subsidize-u-s-health-insurance) and [toxic marketing schemes](https://www.ama.org/publications/eNewsletters/MHSNewsletter/Pages/secrets-next-generation-health-insurance-brand-marketing-directly-consumers.aspx) against any attempt to centralize healthcare\n- Politicians that are catered to by lobbyists of the healthcare entity\n  - Mmmm, [recent corruption](https://psmag.com/news/health-insurance-senate-money-connections)\n\n## Whom does it hurt?\n- The people that pay the healthcare entity for services\n  - By paying more than they have to (more than they should)\n  - See [this](https://www.ibj.com/articles/69394-how-sky-high-deductibles-have-hurt-consumers-and-americas-health-insurance-system) if you're not sold on the idea\n- The people that cannot afford the healthcare entity\n  - Options for this group:\n  - 1.) attempt to afford market\n  - Likely placing themselves in debt during the process\n  - 2.) pick a subsidized plan\n  - Thus contributing to the statistic that most private companies like to throw around (percentage of people on \"Obamacare\", or the ACA)\n  - 3.) go uninsured\n  - Directly influencing higher healthcare prices\n  - Like [this](https://www.healthline.com/health-news/why-you-should-care-if-your-neighbor-doesnt-have-health-insurance#1)\n  - Or [this](https://insight.kellogg.northwestern.edu/article/who-bears-the-cost-of-the-uninsured-nonprofit-hospitals)\n- The employers that compete to offer healthcare options to employees\n\n## Micro-conclusion\n\nI'm hard-pressed to believe that the current healthcare system in the United States is not the by-product of decades of collusion amongst ever-growing entities, the creation of overly-complicated bureaucratic documentation dumped on consumers and a gaming of the current political system. Anyone who can agree with the simplistic, albeit accurate, high-level description of today's system given above should be able to understand my next point. _If_ health insurance providers form a giant entity that collectively represents a pool of money to be used for the greater good of the people that buy into it (that number of people always being statistically the minority of said people mind you) then would it not make sense to eliminate the abstraction of this service across a number of entities and allow for a centralized system to dictate the payout of healthcare needs?\n\nYes, I understand that the current system we endure caters to a nicely structured micro-economy. The market dictates which insurance providers thrive, stick around, and do their **job**. I'm not suggesting that we abolish capitalism and centralize everything that happens to be involved with the greater good of the population. What I am suggesting is that within the context of healthcare, this micro-economy has grown to a size such that the **job** I just mentioned is now ill-defined. Allowing the market to dictate the cost of healthcare is doing nothing but constantly shifting the priority of insurance providers - from providing healthcare costs to the needy - to optimizing profits at all costs.\n\n### Sources\nAll information provided above is editorial content provided by yours truly. Any URLs provided are to sources I find somewhat credible and reparable.\n\nFor the record, I came across many sources that were defending the giant conglomerates we let dictate our healthcare options and decisions. The most compelling I found is [this piece](https://www.verywellhealth.com/health-insurance-companies-unreasonable-profits-1738941) by a site called _Very Well Health_, and perhaps it's no coincidence that it's often healthcare-related organizations that seem to defend insurance agencies the most.\n","fields":{"slug":"/healthcareless/"},"frontmatter":{"date":"June 22, 2018","updated":null,"draft":null,"title":"Healthcareless","hidden":null,"tags":["editorial"]}},{"id":"bd0bad06-dbda-5a90-a78d-585e2290c81c","body":"\nIn the spirit of making a big impact via many small changes, I'd like to spell out the importance of maintaining standards through blacklisting ambiguity wherever possible. It's clear to most savvy developers nowadays that there are a few practices that are a must on any project:\n- Keeping unit tests in mind (and using TDD when it's appropriate)\n- Writing clean code\n- Commenting the necessary amount\n- Conducting efficient code reviews\n\nWhile this is a whitelist of what some can argue should be the bare minimum set of requirements for developing within an enterprise environment, I'd argue there are more:\n- Being fluent in your VCS\n- Leveraging a .gitconfig to its fullest potential, for example\n- Enforcing team-wide standards in terms of branching models & [rebases vs. merges](https://www.atlassian.com/git/tutorials/merging-vs-rebasing)\n- Establishing and recording code format standards - Using an [.editorconfig](https://editorconfig.org/) in each project\n- Developing and maintaining a \"Development Requirements\" document alongside each project\n- Treating each application as if any number of random developers will have to jump in to contribute\n- Making onboarding any new developer to any application as easy as sending them a single URL\n\nKeep in mind that this set of additional standards is only powerful when those abiding by it also enforce the prevention of the opposite of these standards.\n\n## Windows, Etc.\nBroken Windows are a concept brought up within The [Pragmatic Programmer](https://www.amazon.com/Pragmatic-Programmer-Journeyman-Master/dp/020161622X) book, and they essentially represent the disobedience of any of the aforementioned \"standards.\" The idea came from a study that explained neighborhoods immediately begin to \"go downhill,\" so to speak, when a single broken window\nis visible from the street. One broken window is all it takes for the general public to start implementing\ntheir own carelessness and letting socioeconomic ethics fall by the wayside. This is directly analogous to\nteam development endeavors. It takes but a single approved pull (merge) request full of commits that lack comments, break common formatting or are missing details within the commit message before the rest of the team begins rushing their own development endeavors and pushing junk to the remote.\n\nUtilizing the standards spelled out above is not a meaningless, nit-picky feat. Each is imperative in maintaining [velocity](https://www.scruminc.com/velocity/); Within my first year of industry experience I worked on a team with four other developers whose velocity took a noticeable dive just one and a half sprints (~five weeks) after a week of poorly conducted code reviews. Specifically, the team was shedding about twenty story points per sprint from our velocity and the reason was simple. Bad code made it into production due to a rushed code review, which led to prioritizing bug fixes, which led to less time being dedicated to new feature development. When this sort of situation unravels on a development team, while being paired with the usual amount of time- to-market demand, the project spins out of control until it's full of broken windows and velocity is at an all-time low. There goes the neighborhood.\n\n## Don't Be Lazy\nIt cannot be stressed enough that _simple_, effective measures can be taken to measurably improve a team's workflow and code quality. Having a simple standards checklist (like [Joel's list](https://www.joelonsoftware.com/2000/08/09/the-joel-test-12-steps-to-better-code/)) is a great start, but ensuring every member enforces the prevention of practices that don't abide such standards is equally important.\nRespectfully enforce the standards you feel are important\n\n**As a Developer**:\n- Take code reviews as seriously as your own contributions\n- Document as much as you can about your individual processes\n- Publish your local development configurations to a public space\n\n**As a Team**:\n- Establish what it means to conduct a \"good\" code review\n- Follow some [guidelines](https://smartbear.com/learn/code-review/best-practices-for-peer-code-review/) that have already been established, when possible\n- Discuss and document your VCS methodologies\n- Document the development requirements for getting up to speed in contributing to each application\nTL;DR - do not \"break any windows\" by developing habits that are not in line with establishing momentum or adhering to improving development standards. Prevent others from breaking windows. Fix any existing broken windows when you have the time.\n","fields":{"slug":"/broken-windows/"},"frontmatter":{"date":"June 7, 2018","updated":null,"draft":null,"title":"Fixing the Neighborhood by Preventing Broken Windows","hidden":null,"tags":["nerd shit"]}},{"id":"7712828a-d29b-573f-a654-6efe05f7a026","body":"\nDuring my first viewing of [Free Solo](https://play.google.com/store/movies/details?id=lqvbkPjQG-U), the 2018 film about the approach to (and execution of) Alex Honnold's ascent of El Capitan sans equipment, I was humbly reminded of the reason I got into climbing. Honnold's initial attempt resulted in him bailing fairly early on in his climb. While I'm sure there were a few factors that went into this decision, the movie really hammered home the idea that he was giving up because he wasn't ready mentally; Honnold seemed to understand he wasn't executing this particular free solo for the right reasons, the first time around. This made me reflect on why I fell in love with the sport of climbing in the first place.\n\nI personally haven't experienced many other activities that challenge the human spirit in the way that climbing does. Watching Honnold's tenacity in training and oneness with the faces he tackled helped validate the feelings I had when I first began obsessing over bouldering.\n\nOf all his inspirational qualities, though, the one that resonated the strongest was: eating four ingredients out of a pan.\n\nEnter:\n\n# Climber's Hash\n\nHere lies an email sent to a buddy of mine with the perfect recipe for climbers who appreciate pain beyond pulley injuries and tendonitis.\n\n> J,\n\nBelow is my recipe for my climber’s hash. It’s pretty picante, so keep your wits about you and substitute the peppers where necessary.\n\nIngredients:\n> (Numbers are for small/large batch.)\n- 5-10 small red potatoes // sliced length-wise in 1/8” slices\n- 5-10 cloves of garlic // chopped (medium)\n- 1-2 sweet onion // chopped (medium)\n- 2-4 carrots // sliced length-wise in 1/8” slices\n> Peppers:\n> (The Morning After)\n- 1 red fresno // halved length-wise and sliced in 1/16” slices // seeded\n- 2 serrano // sliced length-wise in 1/16” slices // seeded\n- 3 green finger hot // sliced length-wise in 1/16” slices // keep seeds & wax\n- 3 habanero // diced (medium) // keep seeds & wax\n> (Mild)\n- 1 hungarian // halved length-wise and chopped (medium) // keep ~1/2 seeds\n- 1 bell // chopped (medium) // seeded\n- 1-2 heavy handfuls of fresh baby spinach // chopped (medium)\n- 1-2 can(s) butter beans\n- 1-2 tablespoons of coconut oil\n- Salt\n- Pepper\n- Crushed red pepper\n- 2+ tablespoons chili powder\n- 2+ tablespoons cumin\n- 1 teaspoon garlic powder\n\nDirections:\n(Preface: the assumption here is that you’ve cooked before, and/or understand the laws of thermodynamics. Watch the pan & stir fairly regularly during and in-between steps. Furthermore, be sure to wash your hands after dealing with the peppers included in The Morning After variant of the recipe. Under no circumstances should any nose-picking, eye-rubbing or private-part-touching occur until this has been achieved.)\n1. Heat the coconut oil in a large skillet on medium-high heat\n2. Add potatoes & carrots, lightly season with salt and pepper after a minute on heat\n3. Once potatoes & carrots begin browning and softening, add onion, a bit more salt, garlic powder, and a heavy sprinkle of crushed red pepper\n4. Add beans\n5. Once beans begin browning, add peppers & garlic, cumin, and chili powder\n6. Cook for 3-4 minutes\n7. Reduce heat to low and stir in baby spinach\n8. Remove from heat, and enjoy\n\nCorollary: enjoy this meal “Honnold-style” by simply eating it out of the pan you’ve cooked with and whatever utensil you’ve used for stirring\n","fields":{"slug":"/climbers-hash/"},"frontmatter":{"date":"May 5, 2018","updated":null,"draft":null,"title":"Climber's Hash: The Morning After Style","hidden":null,"tags":null}}]}},"pageContext":{}},"staticQueryHashes":[],"slicesMap":{}}